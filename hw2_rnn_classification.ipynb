{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1acf78a",
   "metadata": {
    "id": "b1acf78a"
   },
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 2: Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f64606",
   "metadata": {},
   "source": [
    "### Оценивание и штрафы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c24eb5",
   "metadata": {},
   "source": [
    "\n",
    "Максимально допустимая оценка за работу — __10 (+3) баллов__. Сдавать задание после указанного срока сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн: 5.10.25 23:59__   \n",
    "__Жесткий дедлайн: 8.10.25 23:59__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442a849",
   "metadata": {},
   "source": [
    "### О задании"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "671951a8-2293-4083-afa6-4528ecd945c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.18 (main, Oct  6 2025, 22:29:25) [GCC 15.2.1 20250813]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81c1ab",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит самостоятельно реализовать модель LSTM для решения задачи классификации с пересекающимися классами (multi-label classification). Это вид классификации, в которой каждый объект может относиться одновременно к нескольким классам. Такая задача часто возникает при классификации фильмов по жанрам, научных или новостных статей по темам, музыкальных композиций по инструментам и так далее.\n",
    "\n",
    "В нашем случае мы будем работать с датасетом биотехнических новостей и классифицировать их по темам. Этот датасет уже предобработан: текст приведен к нижнему регистру, удалена пунктуация, все слова разделены проблелом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d928dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1a5fff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "af1a5fff",
    "outputId": "891c58cd-6964-4319-ade3-92bb90356f93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>email rotterdam a former discount supermarket ...</td>\n",
       "      <td>expanding geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>updated 48 mins ago matt cardy getty images fr...</td>\n",
       "      <td>regulatory approval, executive statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>health global ophthalmology devices market ass...</td>\n",
       "      <td>company description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>saturday june 15 2019 young entrepreneur joins...</td>\n",
       "      <td>participation in an event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>share on pinterest share on linkedin escanaba ...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "2103  email rotterdam a former discount supermarket ...   \n",
       "1837  updated 48 mins ago matt cardy getty images fr...   \n",
       "1385  health global ophthalmology devices market ass...   \n",
       "1923  saturday june 15 2019 young entrepreneur joins...   \n",
       "299   share on pinterest share on linkedin escanaba ...   \n",
       "\n",
       "                                        labels  \n",
       "2103                       expanding geography  \n",
       "1837  regulatory approval, executive statement  \n",
       "1385                       company description  \n",
       "1923                 participation in an event  \n",
       "299                                      other  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data/biotech_news.tsv', sep='\\t')\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8afe3be4-8521-4243-b203-99fc84d0251f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "other    484\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset.labels.str.contains('other')].labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbba1d-864c-4b1a-9245-85bdf0c0d286",
   "metadata": {},
   "source": [
    "будем считать, что other это не категория, а отсутствие категории"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HRBZwYd9QMMS",
   "metadata": {
    "id": "HRBZwYd9QMMS"
   },
   "source": [
    "## Предобработка лейблов\n",
    "\n",
    "\n",
    "__Задание 1 (0.5 балла)__. Как вы можете заметить, лейблы записаны в виде строк, разделенных запятыми. Для работы с ними нам нужно преобразовать их в числа. Так как каждый объект может принадлежать нескольким классам, закодируйте лейблы в виде векторов из 0 и 1, где 1 означает, что объект принадлежит соответствующему классу, а 0 – не принадлежит. Имея такую кодировку, мы сможем обучить модель, решая задачу бинарной классификации для каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba33ee8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alliance & partnership': 0,\n",
       " 'article publication': 1,\n",
       " 'clinical trial sponsorship': 2,\n",
       " 'closing': 3,\n",
       " 'company description': 4,\n",
       " 'department establishment': 5,\n",
       " 'event organization': 6,\n",
       " 'executive appointment': 7,\n",
       " 'executive statement': 8,\n",
       " 'expanding geography': 9,\n",
       " 'expanding industry': 10,\n",
       " 'foundation': 11,\n",
       " 'funding round': 12,\n",
       " 'hiring': 13,\n",
       " 'investment in public company': 14,\n",
       " 'ipo exit': 15,\n",
       " 'm&a': 16,\n",
       " 'new initiatives & programs': 17,\n",
       " 'new initiatives or programs': 18,\n",
       " 'participation in an event': 19,\n",
       " 'partnerships & alliances': 20,\n",
       " 'patent publication': 21,\n",
       " 'product launching & presentation': 22,\n",
       " 'product updates': 23,\n",
       " 'regulatory approval': 24,\n",
       " 'service & product providing': 25,\n",
       " 'subsidiary establishment': 26,\n",
       " 'support & philanthropy': 27}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = set()\n",
    "for s in dataset.labels:\n",
    "    labels = s.split(', ')\n",
    "    res = res | set(labels)\n",
    "keys = sorted(list(res))\n",
    "keys.remove('other')\n",
    "labels = dict(zip(keys, range(len(keys))))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f175587-e183-4087-90ad-0010e8b46d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/biotech_news.tsv', sep='\\t')\n",
    "\n",
    "res = set()\n",
    "for s in dataset.labels:\n",
    "    labels = s.split(', ')\n",
    "    res = res | set(labels)\n",
    "keys = sorted(list(res))\n",
    "keys.remove('other')\n",
    "labels = dict(zip(keys, range(len(keys))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1fec761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    def encode_labels(labels_str):\n",
    "        if labels_str == 'other':\n",
    "            return np.zeros(len(keys), dtype=np.float32)\n",
    "\n",
    "        encoded = np.zeros(len(keys), dtype=np.float32)\n",
    "        for label in labels_str.split(', '):\n",
    "            if label in labels:\n",
    "                encoded[labels[label]] = 1.0\n",
    "        return encoded\n",
    "\n",
    "encode_labels('alliance & partnership, closing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c65a9bf-dbe9-4cad-978d-3a0e10b1eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['encoded_labels'] = dataset.labels.apply(encode_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0296f-9699-475e-b4bd-c9e531dca2d4",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vMe0c5AAXM8d",
   "metadata": {
    "id": "vMe0c5AAXM8d"
   },
   "source": [
    "В этом задании мы будем обучать рекуррентные нейронные сети. Как вы знаете, они работают лучше для коротких текстов, так как не очень хорошо улавливают далекие зависимости. Для уменьшение длин текстов их стоит почистить.\n",
    "\n",
    "Сразу разделим выборку на обучающую и тестовую, чтобы считать все нужные статистики только по обучающей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8135000",
   "metadata": {
    "id": "f8135000"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(\n",
    "    dataset.text.values,\n",
    "    dataset.encoded_labels.values,\n",
    "    test_size=0.2,  # do not change this\n",
    "    random_state=0  # do not change this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace679c-db5f-45d3-8fa3-6a5c55eb912a",
   "metadata": {},
   "source": [
    "__Задание 2 (1 балл)__. Удалите из текстов стоп слова, слишком редкие и слишком частые слова. Гиперпараметры подберите самостоятельно (в идеале их стоит подбирать по качеству на тестовой выборке). Если вы считаете, что стоит добавить еще какую-то обработку, то сделайте это. Важно не удалить ничего, что может повлиять на предсказание класса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37132b8",
   "metadata": {},
   "source": [
    "СДЕЛАНО В dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b704d12-cd25-4fe9-b3a3-f01f3c156a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(txt: str):\n",
    "    CUSTOM_FILTERS = [\n",
    "        strip_tags,\n",
    "        strip_punctuation,\n",
    "        strip_multiple_whitespaces,\n",
    "        strip_numeric,\n",
    "        remove_stopwords,\n",
    "    ]\n",
    "    res = preprocess_string(txt, CUSTOM_FILTERS)\n",
    "    res = (stemmer.stem(word) for word in res)\n",
    "    return ' '.join(res)\n",
    "\n",
    "dataset['preprocessed'] = dataset.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1ba6db0-ada4-46ce-809c-325834ed3eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drive plow bone dead olga tokarczuk incred slo...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recent tabl nation budget denel alloc r billio...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>share break good pictur getti metro uk tempt t...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reso current hire posit product managerand mem...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>charter buyer club charter buyer club charter ...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>publish hour ago grate famili present chequ de...</td>\n",
       "      <td>funding round, support &amp; philanthropy, executi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>cenexelcent excel join nearli clinic research ...</td>\n",
       "      <td>clinical trial sponsorship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>jun m pt repli respons ongo covid pandem famil...</td>\n",
       "      <td>new initiatives or programs, funding round, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>whatsapp photo suppli red river wast solut for...</td>\n",
       "      <td>service &amp; product providing, closing, company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>punch brand note includ u s distribut right br...</td>\n",
       "      <td>subsidiary establishment, executive statement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3039 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           preprocessed  \\\n",
       "0     drive plow bone dead olga tokarczuk incred slo...   \n",
       "1     recent tabl nation budget denel alloc r billio...   \n",
       "2     share break good pictur getti metro uk tempt t...   \n",
       "3     reso current hire posit product managerand mem...   \n",
       "4     charter buyer club charter buyer club charter ...   \n",
       "...                                                 ...   \n",
       "3034  publish hour ago grate famili present chequ de...   \n",
       "3035  cenexelcent excel join nearli clinic research ...   \n",
       "3036  jun m pt repli respons ongo covid pandem famil...   \n",
       "3037  whatsapp photo suppli red river wast solut for...   \n",
       "3038  punch brand note includ u s distribut right br...   \n",
       "\n",
       "                                                 labels  \n",
       "0                                                 other  \n",
       "1                                                 other  \n",
       "2                                                 other  \n",
       "3                                                 other  \n",
       "4                                                 other  \n",
       "...                                                 ...  \n",
       "3034  funding round, support & philanthropy, executi...  \n",
       "3035                         clinical trial sponsorship  \n",
       "3036  new initiatives or programs, funding round, ex...  \n",
       "3037  service & product providing, closing, company ...  \n",
       "3038      subsidiary establishment, executive statement  \n",
       "\n",
       "[3039 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[['preprocessed', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fdd5401-da70-4984-97fc-04fb3fbc9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('data/preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4848c2-7fe1-43f9-8564-1144015fc29b",
   "metadata": {},
   "source": [
    "__Задание 3 (1.5 балла)__. Осталось перевести тексты в индексы токенов, чтобы их можно было подавать в модель. У вас есть две опции, как это сделать:\n",
    "1. __(+0 баллов)__ Токенизировать тексты по словам.\n",
    "2. __(до +3 баллов)__ Реализовать свою токенизацию BPE. Количество баллов будет варьироваться в зависимости от эффективности реализации. При реализации нельзя пользоваться специализированными библиотеками.\n",
    "\n",
    "Токенизируйте тексты, переведите их в списки индексов и сложите вместе с лейблами в `DataLoader`. Не забудьте добавить в `DataLoader` `collate_fn`, которая будет дополнять все короткие тексты в батче паддингами. Для маппинга токенов в индексы вам может пригодиться `gensim.corpora.dictionary.Dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa663c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:15<00:00, 161.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:14<00:00, 163.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:00<00:00, 49882.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 608/608 [00:03<00:00, 164.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 608/608 [00:00<00:00, 47966.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from vocab import Vocab\n",
    "from dataset import TextDataset, collate\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "vocab = Vocab(texts_train, 2, 0.7)\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "train_dataset = TextDataset(texts_train, y_train, vocab, device)\n",
    "test_dataset = TextDataset(texts_test, y_test, vocab, device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate(batch, vocab.pad_idx)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate(batch, vocab.pad_idx)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eebfac6e-765b-4f24-9460-7dd837c0e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14331"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2218c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts shape: torch.Size([32, 655])\n",
      "Labels shape: torch.Size([32, 29])\n",
      "Labels dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for batch_texts, batch_labels in train_loader:\n",
    "    print(f\"Texts shape: {batch_texts.shape}\")\n",
    "    print(f\"Labels shape: {batch_labels.shape}\")\n",
    "    print(f\"Labels dtype: {batch_labels.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e78f9-6bef-46f4-8b58-818b7eb0c082",
   "metadata": {},
   "source": [
    "## Метрика качества\n",
    "\n",
    "Перед тем, как приступить к обучению, нам нужно выбрать метрику оценки качества. Так как в задаче классификации с пересекающимися классами классы часто несбалансированы, чаще всего в качестве метрики берется [F1 score](https://en.wikipedia.org/wiki/F-score).\n",
    "\n",
    "Функция `compute_f1` принимает истинные метки и предсказанные и считает среднее значение F1 по всем классам. Используйте ее для оценки качества моделей.\n",
    "\n",
    "$$\n",
    "F1_{total} = \\frac{1}{K} \\sum_{k=1}^K F1(Y_k, \\hat{Y}_k),\n",
    "$$\n",
    "где $Y_k$ – истинные значения для класса k, а $\\hat{Y}_k$ – предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "671a0928-fd68-4f36-bae7-2dacb18fd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1(y_true, y_pred):\n",
    "    assert y_true.ndim == 2\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aagj29J7Ap2H",
   "metadata": {
    "id": "aagj29J7Ap2H"
   },
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae5666",
   "metadata": {
    "id": "56ae5666"
   },
   "source": [
    "### RNN\n",
    "\n",
    "В качестве бейзлайна обучим самую простую рекуррентную нейронную сеть. Напомним, что блок RNN выглядит таким образом.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/yYbNBm6G/tg-image-1635618906.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Его скрытое состояние обновляется по формуле\n",
    "$h_t = \\sigma(W x_{t} + U h_{t-1} + b_h)$. А предсказание считается с помощью применения линейного слоя к последнему токену\n",
    "$o_T = V h_T + b_o$. В качестве функции активации выберите гиперболический тангенс. \n",
    "\n",
    "__Задание 4 (2 балла)__. Реализуйте RNN в соответствии с формулой выше и обучите ее на нашу задачу. Нулевой скрытый вектор инициализируйте нулями, так модель будет обучаться стабильнее, чем при случайной инициализации. После этого замеряйте качество на тестовой выборке. У вас должно получиться значение F1 не меньше 0.33, а само обучение не должно занимать много времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc01c26a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rnn import RNN\n",
    "from deep import LSTM\n",
    "from torchrnn import TorchRNN\n",
    "from vocab import Vocab\n",
    "from vocab_bpe import BPEVocab\n",
    "from dataset import TextDataset, collate, load_biotech, load_preprocessed\n",
    "from train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078a61ec-4dab-448c-86bc-2b17dd24b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "WARMUP_EPOCHS = 12\n",
    "EPOCHS = 72\n",
    "LR = 3e-3\n",
    "WD = 1e-3\n",
    "\n",
    "FACTOR = 0.5\n",
    "PATIENCE = 5\n",
    "\n",
    "config = {\n",
    "    'embed_dim': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_classes': 28,\n",
    "    'dropout_rate_emb': 0.1,\n",
    "    'dropout_rate_linear': 0.5,\n",
    "    'dropout_rate_hidden': 0.1,\n",
    "    # 'num_layers': 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b25a882-72e8-4e50-ac94-2d2c1507979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:00<00:00, 3044.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3646\n",
      "Tokenizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:00<00:00, 4992.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:00<00:00, 51785.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 608/608 [00:00<00:00, 5112.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 608/608 [00:00<00:00, 47645.63it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset...')\n",
    "# dataset, labels = load_biotech()\n",
    "dataset, labels = load_preprocessed()\n",
    "\n",
    "device = 'cuda:0'\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(\n",
    "    dataset.text.values,\n",
    "    dataset.encoded_labels.values,\n",
    "    test_size=0.2,  # do not change this\n",
    "    random_state=0  # do not change this\n",
    ")\n",
    "\n",
    "print('Building vocab...')\n",
    "vocab = Vocab(texts_train, no_below=26, no_above=0.68, max_tokens=36, clean=False)\n",
    "# vocab = BPEVocab(texts_train, vocab_size=3000, max_tokens=36, clean=False)\n",
    "\n",
    "print('Vocab size:', len(vocab.dictionary))\n",
    "\n",
    "print('Tokenizing ...')\n",
    "train_dataset = TextDataset(texts_train, y_train, vocab, device)\n",
    "test_dataset = TextDataset(texts_test, y_test, vocab, device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate(batch, vocab.pad_idx)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate(batch, vocab.pad_idx)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb5b727-08eb-4f1e-bb5b-a5cec2d6fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "│ Epoch 01 │ Train Loss: 1.3742 │ Val Loss: 1.3961 │ F1: 8.69% │ LR: 0.00027750 │\n",
      "│ Epoch 02 │ Train Loss: 1.3500 │ Val Loss: 1.3825 │ F1: 8.70% │ LR: 0.00052500 │\n",
      "│ Epoch 03 │ Train Loss: 1.3205 │ Val Loss: 1.3776 │ F1: 8.49% │ LR: 0.00077250 │\n",
      "│ Epoch 04 │ Train Loss: 1.2979 │ Val Loss: 1.4439 │ F1: 8.80% │ LR: 0.00102000 │\n",
      "│ Epoch 05 │ Train Loss: 1.2656 │ Val Loss: 1.4130 │ F1: 8.45% │ LR: 0.00126750 │\n",
      "│ Epoch 06 │ Train Loss: 1.2405 │ Val Loss: 1.3529 │ F1: 9.24% │ LR: 0.00151500 │\n",
      "│ Epoch 07 │ Train Loss: 1.2315 │ Val Loss: 1.6068 │ F1: 8.24% │ LR: 0.00176250 │\n",
      "│ Epoch 08 │ Train Loss: 1.1811 │ Val Loss: 1.5093 │ F1: 10.65% │ LR: 0.00201000 │\n",
      "│ Epoch 09 │ Train Loss: 1.1700 │ Val Loss: 1.4442 │ F1: 9.88% │ LR: 0.00225750 │\n",
      "│ Epoch 10 │ Train Loss: 1.1583 │ Val Loss: 1.4757 │ F1: 11.76% │ LR: 0.00250500 │\n",
      "│ Epoch 11 │ Train Loss: 1.1030 │ Val Loss: 1.4144 │ F1: 10.94% │ LR: 0.00275250 │\n",
      "│ Epoch 12 │ Train Loss: 1.1230 │ Val Loss: 1.3668 │ F1: 11.19% │ LR: 0.00300000 │\n",
      "│ Epoch 13 │ Train Loss: 1.0273 │ Val Loss: 1.5480 │ F1: 18.25% │ LR: 0.00300000 │\n",
      "│ Epoch 14 │ Train Loss: 0.9971 │ Val Loss: 1.5244 │ F1: 16.63% │ LR: 0.00300000 │\n",
      "│ Epoch 15 │ Train Loss: 0.9279 │ Val Loss: 1.7741 │ F1: 21.23% │ LR: 0.00300000 │\n",
      "│ Epoch 16 │ Train Loss: 0.9133 │ Val Loss: 1.5401 │ F1: 19.28% │ LR: 0.00300000 │\n",
      "│ Epoch 17 │ Train Loss: 0.9111 │ Val Loss: 1.5002 │ F1: 20.93% │ LR: 0.00300000 │\n",
      "│ Epoch 18 │ Train Loss: 0.8164 │ Val Loss: 1.6934 │ F1: 23.11% │ LR: 0.00300000 │\n",
      "│ Epoch 19 │ Train Loss: 0.8405 │ Val Loss: 1.5083 │ F1: 17.76% │ LR: 0.00300000 │\n",
      "│ Epoch 20 │ Train Loss: 0.8546 │ Val Loss: 1.7512 │ F1: 25.51% │ LR: 0.00300000 │\n",
      "│ Epoch 21 │ Train Loss: 0.8423 │ Val Loss: 1.8821 │ F1: 26.84% │ LR: 0.00300000 │\n",
      "│ Epoch 22 │ Train Loss: 0.7731 │ Val Loss: 1.7784 │ F1: 24.04% │ LR: 0.00300000 │\n",
      "│ Epoch 23 │ Train Loss: 0.7848 │ Val Loss: 1.7652 │ F1: 23.74% │ LR: 0.00300000 │\n",
      "│ Epoch 24 │ Train Loss: 0.7459 │ Val Loss: 2.0811 │ F1: 30.41% │ LR: 0.00300000 │\n",
      "│ Epoch 25 │ Train Loss: 0.7512 │ Val Loss: 2.0948 │ F1: 28.53% │ LR: 0.00300000 │\n",
      "│ Epoch 26 │ Train Loss: 0.7259 │ Val Loss: 2.0080 │ F1: 28.84% │ LR: 0.00300000 │\n",
      "│ Epoch 27 │ Train Loss: 0.7176 │ Val Loss: 2.0074 │ F1: 28.33% │ LR: 0.00300000 │\n",
      "│ Epoch 28 │ Train Loss: 0.7510 │ Val Loss: 1.9226 │ F1: 27.37% │ LR: 0.00300000 │\n",
      "│ Epoch 29 │ Train Loss: 0.7598 │ Val Loss: 2.1230 │ F1: 31.30% │ LR: 0.00150000 │\n",
      "│ Epoch 30 │ Train Loss: 0.7340 │ Val Loss: 2.0739 │ F1: 29.57% │ LR: 0.00150000 │\n",
      "│ Epoch 31 │ Train Loss: 0.7023 │ Val Loss: 2.2554 │ F1: 32.27% │ LR: 0.00150000 │\n",
      "│ Epoch 32 │ Train Loss: 0.7196 │ Val Loss: 2.2837 │ F1: 32.76% │ LR: 0.00150000 │\n",
      "│ Epoch 33 │ Train Loss: 0.7064 │ Val Loss: 2.2703 │ F1: 31.81% │ LR: 0.00150000 │\n",
      "│ Epoch 34 │ Train Loss: 0.7083 │ Val Loss: 2.2769 │ F1: 32.98% │ LR: 0.00150000 │\n",
      "│ Epoch 35 │ Train Loss: 0.7117 │ Val Loss: 2.4668 │ F1: 33.60% │ LR: 0.00150000 │\n",
      "│ Epoch 36 │ Train Loss: 0.7158 │ Val Loss: 2.3464 │ F1: 33.22% │ LR: 0.00150000 │\n",
      "│ Epoch 37 │ Train Loss: 0.6983 │ Val Loss: 2.2721 │ F1: 32.16% │ LR: 0.00150000 │\n",
      "│ Epoch 38 │ Train Loss: 0.7275 │ Val Loss: 2.3069 │ F1: 32.85% │ LR: 0.00150000 │\n",
      "│ Epoch 39 │ Train Loss: 0.6924 │ Val Loss: 2.4564 │ F1: 32.96% │ LR: 0.00150000 │\n",
      "│ Epoch 40 │ Train Loss: 0.7189 │ Val Loss: 2.2012 │ F1: 31.23% │ LR: 0.00075000 │\n",
      "│ Epoch 41 │ Train Loss: 0.6915 │ Val Loss: 2.5679 │ F1: 33.88% │ LR: 0.00075000 │\n",
      "│ Epoch 42 │ Train Loss: 0.7172 │ Val Loss: 2.3133 │ F1: 31.74% │ LR: 0.00075000 │\n",
      "│ Epoch 43 │ Train Loss: 0.6842 │ Val Loss: 2.3694 │ F1: 32.52% │ LR: 0.00075000 │\n",
      "│ Epoch 44 │ Train Loss: 0.6827 │ Val Loss: 2.4772 │ F1: 33.10% │ LR: 0.00075000 │\n",
      "│ Epoch 45 │ Train Loss: 0.7247 │ Val Loss: 2.2159 │ F1: 30.99% │ LR: 0.00075000 │\n",
      "│ Epoch 46 │ Train Loss: 0.6827 │ Val Loss: 2.5537 │ F1: 33.30% │ LR: 0.00037500 │\n",
      "│ Epoch 47 │ Train Loss: 0.6721 │ Val Loss: 2.3250 │ F1: 32.16% │ LR: 0.00037500 │\n",
      "│ Epoch 48 │ Train Loss: 0.6785 │ Val Loss: 2.4759 │ F1: 33.19% │ LR: 0.00037500 │\n",
      "│ Epoch 49 │ Train Loss: 0.7056 │ Val Loss: 2.3963 │ F1: 33.12% │ LR: 0.00037500 │\n",
      "│ Epoch 50 │ Train Loss: 0.7114 │ Val Loss: 2.4353 │ F1: 33.25% │ LR: 0.00018750 │\n",
      "│ Epoch 51 │ Train Loss: 0.6556 │ Val Loss: 2.4440 │ F1: 33.01% │ LR: 0.00018750 │\n",
      "│ Epoch 52 │ Train Loss: 0.7001 │ Val Loss: 2.4741 │ F1: 33.34% │ LR: 0.00018750 │\n",
      "│ Epoch 53 │ Train Loss: 0.6948 │ Val Loss: 2.3934 │ F1: 33.11% │ LR: 0.00018750 │\n",
      "│ Epoch 54 │ Train Loss: 0.6877 │ Val Loss: 2.4524 │ F1: 33.08% │ LR: 0.00009375 │\n",
      "│ Epoch 55 │ Train Loss: 0.6999 │ Val Loss: 2.4912 │ F1: 33.48% │ LR: 0.00009375 │\n",
      "│ Epoch 56 │ Train Loss: 0.7133 │ Val Loss: 2.4965 │ F1: 33.52% │ LR: 0.00009375 │\n",
      "│ Epoch 57 │ Train Loss: 0.7065 │ Val Loss: 2.4702 │ F1: 33.40% │ LR: 0.00009375 │\n",
      "│ Epoch 58 │ Train Loss: 0.6774 │ Val Loss: 2.4655 │ F1: 33.40% │ LR: 0.00004688 │\n",
      "│ Epoch 59 │ Train Loss: 0.7109 │ Val Loss: 2.4673 │ F1: 33.42% │ LR: 0.00004688 │\n",
      "│ Epoch 60 │ Train Loss: 0.7208 │ Val Loss: 2.4624 │ F1: 33.24% │ LR: 0.00004688 │\n",
      "│ Epoch 61 │ Train Loss: 0.6837 │ Val Loss: 2.4621 │ F1: 33.19% │ LR: 0.00004688 │\n",
      "│ Epoch 62 │ Train Loss: 0.6928 │ Val Loss: 2.4603 │ F1: 33.23% │ LR: 0.00002344 │\n",
      "│ Epoch 63 │ Train Loss: 0.6744 │ Val Loss: 2.4654 │ F1: 33.20% │ LR: 0.00002344 │\n",
      "│ Epoch 64 │ Train Loss: 0.6621 │ Val Loss: 2.4798 │ F1: 33.26% │ LR: 0.00002344 │\n",
      "│ Epoch 65 │ Train Loss: 0.6852 │ Val Loss: 2.4849 │ F1: 33.51% │ LR: 0.00002344 │\n",
      "│ Epoch 66 │ Train Loss: 0.7057 │ Val Loss: 2.4821 │ F1: 33.35% │ LR: 0.00001172 │\n",
      "│ Epoch 67 │ Train Loss: 0.6793 │ Val Loss: 2.4792 │ F1: 33.29% │ LR: 0.00001172 │\n",
      "│ Epoch 68 │ Train Loss: 0.6893 │ Val Loss: 2.4761 │ F1: 33.23% │ LR: 0.00001172 │\n",
      "│ Epoch 69 │ Train Loss: 0.6827 │ Val Loss: 2.4703 │ F1: 33.18% │ LR: 0.00001172 │\n",
      "│ Epoch 70 │ Train Loss: 0.6893 │ Val Loss: 2.4706 │ F1: 33.18% │ LR: 0.00000586 │\n",
      "│ Epoch 71 │ Train Loss: 0.6941 │ Val Loss: 2.4709 │ F1: 33.18% │ LR: 0.00000586 │\n",
      "│ Epoch 72 │ Train Loss: 0.6841 │ Val Loss: 2.4724 │ F1: 33.18% │ LR: 0.00000586 │\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor((y_train.shape[0] - y_train.sum(0)) / (y_train.sum(0) + 1e-6), dtype=torch.float32, device=device) # type: ignore\n",
    "# class_weights = torch.tensor((y_train.shape[0]) / (y_train.sum(0) + 1e-6), dtype=torch.float32, device=device)\n",
    "\n",
    "config['vocab_size'] = len(vocab.dictionary)\n",
    "config['pad_idx'] = vocab.pad_idx\n",
    "\n",
    "gt = torch.stack([torch.tensor(label) for label in y_test], dim=0).cpu()\n",
    "model = RNN(**config).to(device)\n",
    "\n",
    "train_config = {\n",
    "    'warmup_epochs': WARMUP_EPOCHS,\n",
    "    'epochs': EPOCHS,\n",
    "    'model': model,\n",
    "    'optim': AdamW(model.parameters(), lr=LR, weight_decay=WD),\n",
    "    'criterion': BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'val_gt': gt,\n",
    "}\n",
    "\n",
    "\n",
    "total_warmup_steps = WARMUP_EPOCHS * len(train_loader) \n",
    "\n",
    "warmup_scheduler = lr_scheduler.LinearLR(\n",
    "    train_config['optim'],\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=total_warmup_steps\n",
    ")\n",
    "\n",
    "\n",
    "plateau_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    train_config['optim'],\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "train_config['warmup_scheduler'] = warmup_scheduler\n",
    "train_config['plateau_scheduler'] = plateau_scheduler\n",
    "\n",
    "print('Training model...')\n",
    "train(**train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd8006-ef8d-4830-bc40-472075debab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "xqt0dk6LEJUU",
   "metadata": {
    "id": "xqt0dk6LEJUU"
   },
   "source": [
    "### LSTM\n",
    "\n",
    "<img src=\"https://i.postimg.cc/pL5LdmpL/tg-image-2290675322.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Теперь перейдем к более продвинутым рекурренным моделям, а именно LSTM. Из-за дополнительного вектора памяти эта модель должна гораздо лучше улавливать далекие зависимости, что должно напрямую отражаться на качестве.\n",
    "\n",
    "Параметры блока LSTM обновляются вот так ($\\sigma$ означает сигмоиду):\n",
    "\\begin{align}\n",
    "f_{t} &= \\sigma(W_f x_{t} + U_f h_{t-1} + b_f) \\\\ \n",
    "i_{t} &= \\sigma(W_i x_{t} + U_i h_{t-1} + b_i) \\\\\n",
    "\\tilde{c}_{t} &= \\tanh(W_c x_{t} + U_c h_{t-1} + b_i) \\\\\n",
    "c_{t} &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\n",
    "o_{t} &= \\sigma(W_t x_{t} + U_t h_{t-1} + b_t) \\\\\n",
    "h_t &= o_t \\odot \\tanh(c_t)\n",
    "\\end{align}\n",
    "\n",
    "__Задание 5 (2 балла).__ Реализуйте LSTM по описанной схеме. Выберите гиперпараметры LSTM так, чтобы их общее число (без учета слоя эмбеддингов) примерно совпадало с числом параметров обычной RNN, но размерность скрытого слоя была не меньше 64. Так мы будем сравнивать архитектуры максимально независимо. Обучите LSTM до сходимости и сравните качество с RNN на тестовой выборке. Удалось ли получить лучший результат? Как вы можете это объяснить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e18b79b-f2c6-4474-a5c0-c8ce51f13afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "│ Epoch 01 │ Train Loss: 1.2922 │ Val Loss: 1.3910 │ F1: 8.43% │ LR: 0.00027750 │\n",
      "│ Epoch 02 │ Train Loss: 1.3045 │ Val Loss: 1.3899 │ F1: 9.30% │ LR: 0.00052500 │\n",
      "│ Epoch 03 │ Train Loss: 1.2907 │ Val Loss: 1.3879 │ F1: 10.08% │ LR: 0.00077250 │\n",
      "│ Epoch 04 │ Train Loss: 1.2903 │ Val Loss: 1.3849 │ F1: 10.97% │ LR: 0.00102000 │\n",
      "│ Epoch 05 │ Train Loss: 1.2639 │ Val Loss: 1.4865 │ F1: 9.11% │ LR: 0.00126750 │\n",
      "│ Epoch 06 │ Train Loss: 1.2410 │ Val Loss: 1.3208 │ F1: 11.35% │ LR: 0.00151500 │\n",
      "│ Epoch 07 │ Train Loss: 1.1548 │ Val Loss: 1.3142 │ F1: 13.66% │ LR: 0.00176250 │\n",
      "│ Epoch 08 │ Train Loss: 1.1201 │ Val Loss: 1.3037 │ F1: 14.55% │ LR: 0.00201000 │\n",
      "│ Epoch 09 │ Train Loss: 1.0852 │ Val Loss: 1.2112 │ F1: 13.57% │ LR: 0.00225750 │\n",
      "│ Epoch 10 │ Train Loss: 1.0847 │ Val Loss: 1.4101 │ F1: 14.37% │ LR: 0.00250500 │\n",
      "│ Epoch 11 │ Train Loss: 0.9826 │ Val Loss: 1.2518 │ F1: 15.16% │ LR: 0.00275250 │\n",
      "│ Epoch 12 │ Train Loss: 0.9863 │ Val Loss: 1.3732 │ F1: 17.21% │ LR: 0.00300000 │\n",
      "│ Epoch 13 │ Train Loss: 0.9632 │ Val Loss: 1.3051 │ F1: 19.35% │ LR: 0.00300000 │\n",
      "│ Epoch 14 │ Train Loss: 0.9590 │ Val Loss: 1.3819 │ F1: 21.51% │ LR: 0.00300000 │\n",
      "│ Epoch 15 │ Train Loss: 0.9103 │ Val Loss: 1.3765 │ F1: 22.19% │ LR: 0.00300000 │\n",
      "│ Epoch 16 │ Train Loss: 0.8517 │ Val Loss: 1.5302 │ F1: 19.84% │ LR: 0.00300000 │\n",
      "│ Epoch 17 │ Train Loss: 0.9012 │ Val Loss: 1.4329 │ F1: 24.06% │ LR: 0.00300000 │\n",
      "│ Epoch 18 │ Train Loss: 0.8649 │ Val Loss: 1.3939 │ F1: 23.26% │ LR: 0.00300000 │\n",
      "│ Epoch 19 │ Train Loss: 0.8247 │ Val Loss: 1.4869 │ F1: 25.50% │ LR: 0.00300000 │\n",
      "│ Epoch 20 │ Train Loss: 0.8247 │ Val Loss: 1.3420 │ F1: 24.86% │ LR: 0.00300000 │\n",
      "│ Epoch 21 │ Train Loss: 0.8130 │ Val Loss: 1.5082 │ F1: 26.26% │ LR: 0.00300000 │\n",
      "│ Epoch 22 │ Train Loss: 0.8201 │ Val Loss: 1.5235 │ F1: 28.04% │ LR: 0.00300000 │\n",
      "│ Epoch 23 │ Train Loss: 0.8089 │ Val Loss: 1.4935 │ F1: 24.13% │ LR: 0.00300000 │\n",
      "│ Epoch 24 │ Train Loss: 0.8122 │ Val Loss: 1.5115 │ F1: 28.35% │ LR: 0.00300000 │\n",
      "│ Epoch 25 │ Train Loss: 0.7768 │ Val Loss: 1.6054 │ F1: 26.99% │ LR: 0.00300000 │\n",
      "│ Epoch 26 │ Train Loss: 0.7483 │ Val Loss: 1.4680 │ F1: 28.84% │ LR: 0.00300000 │\n",
      "│ Epoch 27 │ Train Loss: 0.7408 │ Val Loss: 1.5540 │ F1: 28.76% │ LR: 0.00300000 │\n",
      "│ Epoch 28 │ Train Loss: 0.7394 │ Val Loss: 1.6044 │ F1: 30.28% │ LR: 0.00300000 │\n",
      "│ Epoch 29 │ Train Loss: 0.7508 │ Val Loss: 1.7612 │ F1: 33.09% │ LR: 0.00300000 │\n",
      "│ Epoch 30 │ Train Loss: 0.7637 │ Val Loss: 1.8812 │ F1: 34.29% │ LR: 0.00300000 │\n",
      "│ Epoch 31 │ Train Loss: 0.7637 │ Val Loss: 1.9376 │ F1: 34.01% │ LR: 0.00300000 │\n",
      "│ Epoch 32 │ Train Loss: 0.7314 │ Val Loss: 1.9528 │ F1: 35.61% │ LR: 0.00300000 │\n",
      "│ Epoch 33 │ Train Loss: 0.7161 │ Val Loss: 1.8099 │ F1: 34.11% │ LR: 0.00300000 │\n",
      "│ Epoch 34 │ Train Loss: 0.6807 │ Val Loss: 1.9255 │ F1: 34.82% │ LR: 0.00300000 │\n",
      "│ Epoch 35 │ Train Loss: 0.7240 │ Val Loss: 1.9945 │ F1: 35.91% │ LR: 0.00300000 │\n",
      "│ Epoch 36 │ Train Loss: 0.6774 │ Val Loss: 2.0388 │ F1: 36.30% │ LR: 0.00300000 │\n",
      "│ Epoch 37 │ Train Loss: 0.7068 │ Val Loss: 1.8374 │ F1: 34.79% │ LR: 0.00300000 │\n",
      "│ Epoch 38 │ Train Loss: 0.7061 │ Val Loss: 1.8571 │ F1: 36.01% │ LR: 0.00300000 │\n",
      "│ Epoch 39 │ Train Loss: 0.7083 │ Val Loss: 2.0816 │ F1: 37.74% │ LR: 0.00300000 │\n",
      "│ Epoch 40 │ Train Loss: 0.7290 │ Val Loss: 2.0981 │ F1: 39.11% │ LR: 0.00300000 │\n",
      "│ Epoch 41 │ Train Loss: 0.6742 │ Val Loss: 2.3524 │ F1: 36.60% │ LR: 0.00300000 │\n",
      "│ Epoch 42 │ Train Loss: 0.6994 │ Val Loss: 2.0721 │ F1: 35.97% │ LR: 0.00300000 │\n",
      "│ Epoch 43 │ Train Loss: 0.6831 │ Val Loss: 2.0407 │ F1: 36.67% │ LR: 0.00300000 │\n",
      "│ Epoch 44 │ Train Loss: 0.6931 │ Val Loss: 2.2487 │ F1: 36.73% │ LR: 0.00300000 │\n",
      "│ Epoch 45 │ Train Loss: 0.6641 │ Val Loss: 2.3840 │ F1: 37.16% │ LR: 0.00150000 │\n",
      "│ Epoch 46 │ Train Loss: 0.6888 │ Val Loss: 2.3488 │ F1: 38.63% │ LR: 0.00150000 │\n",
      "│ Epoch 47 │ Train Loss: 0.6537 │ Val Loss: 2.6103 │ F1: 39.28% │ LR: 0.00150000 │\n",
      "│ Epoch 48 │ Train Loss: 0.6857 │ Val Loss: 2.3807 │ F1: 39.61% │ LR: 0.00150000 │\n",
      "│ Epoch 49 │ Train Loss: 0.6871 │ Val Loss: 2.3463 │ F1: 39.50% │ LR: 0.00150000 │\n",
      "│ Epoch 50 │ Train Loss: 0.6618 │ Val Loss: 2.4229 │ F1: 40.09% │ LR: 0.00150000 │\n",
      "│ Epoch 51 │ Train Loss: 0.7137 │ Val Loss: 2.5283 │ F1: 39.52% │ LR: 0.00150000 │\n",
      "│ Epoch 52 │ Train Loss: 0.6853 │ Val Loss: 2.7791 │ F1: 40.51% │ LR: 0.00150000 │\n",
      "│ Epoch 53 │ Train Loss: 0.6756 │ Val Loss: 2.4383 │ F1: 39.15% │ LR: 0.00150000 │\n",
      "│ Epoch 54 │ Train Loss: 0.6617 │ Val Loss: 2.5821 │ F1: 40.40% │ LR: 0.00150000 │\n",
      "│ Epoch 55 │ Train Loss: 0.6814 │ Val Loss: 2.5452 │ F1: 41.57% │ LR: 0.00150000 │\n",
      "│ Epoch 56 │ Train Loss: 0.6864 │ Val Loss: 2.5541 │ F1: 40.37% │ LR: 0.00150000 │\n",
      "│ Epoch 57 │ Train Loss: 0.6786 │ Val Loss: 2.7309 │ F1: 40.77% │ LR: 0.00150000 │\n",
      "│ Epoch 58 │ Train Loss: 0.6634 │ Val Loss: 2.7856 │ F1: 41.14% │ LR: 0.00150000 │\n",
      "│ Epoch 59 │ Train Loss: 0.6729 │ Val Loss: 2.7274 │ F1: 41.01% │ LR: 0.00150000 │\n",
      "│ Epoch 60 │ Train Loss: 0.6741 │ Val Loss: 2.7494 │ F1: 40.50% │ LR: 0.00075000 │\n",
      "│ Epoch 61 │ Train Loss: 0.6796 │ Val Loss: 2.8508 │ F1: 40.88% │ LR: 0.00075000 │\n",
      "│ Epoch 62 │ Train Loss: 0.6831 │ Val Loss: 2.8067 │ F1: 41.03% │ LR: 0.00075000 │\n",
      "│ Epoch 63 │ Train Loss: 0.6678 │ Val Loss: 2.9051 │ F1: 41.83% │ LR: 0.00075000 │\n",
      "│ Epoch 64 │ Train Loss: 0.6513 │ Val Loss: 3.0136 │ F1: 40.17% │ LR: 0.00075000 │\n",
      "│ Epoch 65 │ Train Loss: 0.6823 │ Val Loss: 2.9908 │ F1: 41.04% │ LR: 0.00075000 │\n",
      "│ Epoch 66 │ Train Loss: 0.6748 │ Val Loss: 3.0374 │ F1: 40.53% │ LR: 0.00075000 │\n",
      "│ Epoch 67 │ Train Loss: 0.6617 │ Val Loss: 3.0383 │ F1: 41.12% │ LR: 0.00075000 │\n",
      "│ Epoch 68 │ Train Loss: 0.6431 │ Val Loss: 2.9475 │ F1: 41.53% │ LR: 0.00037500 │\n",
      "│ Epoch 69 │ Train Loss: 0.7009 │ Val Loss: 2.9764 │ F1: 40.37% │ LR: 0.00037500 │\n",
      "│ Epoch 70 │ Train Loss: 0.6429 │ Val Loss: 2.9843 │ F1: 40.67% │ LR: 0.00037500 │\n",
      "│ Epoch 71 │ Train Loss: 0.6350 │ Val Loss: 3.1224 │ F1: 40.73% │ LR: 0.00037500 │\n",
      "│ Epoch 72 │ Train Loss: 0.6284 │ Val Loss: 3.1331 │ F1: 40.84% │ LR: 0.00018750 │\n"
     ]
    }
   ],
   "source": [
    "from lstm import LSTM\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "WARMUP_EPOCHS = 12\n",
    "EPOCHS = 72\n",
    "LR = 3e-3\n",
    "WD = 1e-3\n",
    "\n",
    "FACTOR = 0.5\n",
    "PATIENCE = 5\n",
    "\n",
    "config = {\n",
    "    'embed_dim': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_classes': 28,\n",
    "    'dropout_rate_emb': 0.1,\n",
    "    'dropout_rate_linear': 0.5,\n",
    "    'dropout_rate_hidden': 0.1,\n",
    "    # 'num_layers': 2\n",
    "}\n",
    "\n",
    "config['vocab_size'] = len(vocab.dictionary)\n",
    "config['pad_idx'] = vocab.pad_idx\n",
    "\n",
    "gt = torch.stack([torch.tensor(label) for label in y_test], dim=0).cpu()\n",
    "model = LSTM(**config).to(device)\n",
    "\n",
    "train_config = {\n",
    "    'warmup_epochs': WARMUP_EPOCHS,\n",
    "    'epochs': EPOCHS,\n",
    "    'model': model,\n",
    "    'optim': AdamW(model.parameters(), lr=LR, weight_decay=WD),\n",
    "    'criterion': BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'val_gt': gt,\n",
    "}\n",
    "\n",
    "\n",
    "total_warmup_steps = WARMUP_EPOCHS * len(train_loader) \n",
    "\n",
    "warmup_scheduler = lr_scheduler.LinearLR(\n",
    "    train_config['optim'],\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=total_warmup_steps\n",
    ")\n",
    "\n",
    "\n",
    "plateau_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    train_config['optim'],\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "train_config['warmup_scheduler'] = warmup_scheduler\n",
    "train_config['plateau_scheduler'] = plateau_scheduler\n",
    "\n",
    "print('Training model...')\n",
    "train(**train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfdd1a1-51f4-4065-85f7-22ed773a2628",
   "metadata": {},
   "source": [
    "__Задание 6 (2 балла).__ Главный недостаток RNN моделей заключается в том, что при сжатии всей информации в один вектор, важные детали пропадают. Для решения этой проблемы был придуман механизм внимания. Реализуйте его по [оригинальной статье](https://arxiv.org/abs/1409.0473). Замерьте качество и сделайте выводы.   \n",
    "Обратите внимание, что метод был предложен для Encoder-Decoder моделей. В нашем случае декодера нет, поэтому встройте внимание в энкодер: каждый блок LSTM будет смотреть на выходы всех предыдущих блоков.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5bd1fa9-2c1f-4268-be24-5c31752204ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "│ Epoch 01 │ Train Loss: 1.3001 │ Val Loss: 1.3925 │ F1: 9.40% │ LR: 0.00027750 │\n",
      "│ Epoch 02 │ Train Loss: 1.3044 │ Val Loss: 1.3919 │ F1: 10.84% │ LR: 0.00052500 │\n",
      "│ Epoch 03 │ Train Loss: 1.3004 │ Val Loss: 1.3899 │ F1: 11.29% │ LR: 0.00077250 │\n",
      "│ Epoch 04 │ Train Loss: 1.2821 │ Val Loss: 1.5080 │ F1: 8.77% │ LR: 0.00102000 │\n",
      "│ Epoch 05 │ Train Loss: 1.2336 │ Val Loss: 1.3939 │ F1: 11.02% │ LR: 0.00126750 │\n",
      "│ Epoch 06 │ Train Loss: 1.1727 │ Val Loss: 1.5227 │ F1: 13.14% │ LR: 0.00151500 │\n",
      "│ Epoch 07 │ Train Loss: 1.1366 │ Val Loss: 1.3618 │ F1: 12.22% │ LR: 0.00176250 │\n",
      "│ Epoch 08 │ Train Loss: 1.1556 │ Val Loss: 1.5737 │ F1: 12.55% │ LR: 0.00201000 │\n",
      "│ Epoch 09 │ Train Loss: 1.0867 │ Val Loss: 1.4545 │ F1: 13.00% │ LR: 0.00225750 │\n",
      "│ Epoch 10 │ Train Loss: 1.0588 │ Val Loss: 1.7775 │ F1: 14.09% │ LR: 0.00250500 │\n",
      "│ Epoch 11 │ Train Loss: 1.0500 │ Val Loss: 1.5078 │ F1: 14.85% │ LR: 0.00275250 │\n",
      "│ Epoch 12 │ Train Loss: 0.9818 │ Val Loss: 1.5781 │ F1: 16.15% │ LR: 0.00300000 │\n",
      "│ Epoch 13 │ Train Loss: 0.9523 │ Val Loss: 1.5597 │ F1: 17.29% │ LR: 0.00300000 │\n",
      "│ Epoch 14 │ Train Loss: 0.9264 │ Val Loss: 1.4626 │ F1: 18.37% │ LR: 0.00300000 │\n",
      "│ Epoch 15 │ Train Loss: 0.9494 │ Val Loss: 1.7489 │ F1: 16.78% │ LR: 0.00300000 │\n",
      "│ Epoch 16 │ Train Loss: 0.9309 │ Val Loss: 1.5641 │ F1: 21.25% │ LR: 0.00300000 │\n",
      "│ Epoch 17 │ Train Loss: 0.9248 │ Val Loss: 1.7206 │ F1: 20.46% │ LR: 0.00300000 │\n",
      "│ Epoch 18 │ Train Loss: 0.9273 │ Val Loss: 1.5897 │ F1: 20.23% │ LR: 0.00300000 │\n",
      "│ Epoch 19 │ Train Loss: 0.8857 │ Val Loss: 1.8468 │ F1: 20.29% │ LR: 0.00300000 │\n",
      "│ Epoch 20 │ Train Loss: 0.8762 │ Val Loss: 1.9875 │ F1: 22.76% │ LR: 0.00300000 │\n",
      "│ Epoch 21 │ Train Loss: 0.8397 │ Val Loss: 1.8376 │ F1: 24.95% │ LR: 0.00300000 │\n",
      "│ Epoch 22 │ Train Loss: 0.8284 │ Val Loss: 1.7161 │ F1: 20.89% │ LR: 0.00300000 │\n",
      "│ Epoch 23 │ Train Loss: 0.8353 │ Val Loss: 1.7546 │ F1: 22.49% │ LR: 0.00300000 │\n",
      "│ Epoch 24 │ Train Loss: 0.8183 │ Val Loss: 2.3065 │ F1: 22.91% │ LR: 0.00300000 │\n",
      "│ Epoch 25 │ Train Loss: 0.8124 │ Val Loss: 2.1523 │ F1: 23.75% │ LR: 0.00300000 │\n",
      "│ Epoch 26 │ Train Loss: 0.7980 │ Val Loss: 2.1584 │ F1: 24.35% │ LR: 0.00150000 │\n",
      "│ Epoch 27 │ Train Loss: 0.8006 │ Val Loss: 2.5508 │ F1: 26.67% │ LR: 0.00150000 │\n",
      "│ Epoch 28 │ Train Loss: 0.7882 │ Val Loss: 2.5205 │ F1: 26.52% │ LR: 0.00150000 │\n",
      "│ Epoch 29 │ Train Loss: 0.7546 │ Val Loss: 2.7831 │ F1: 28.04% │ LR: 0.00150000 │\n",
      "│ Epoch 30 │ Train Loss: 0.7204 │ Val Loss: 2.8261 │ F1: 27.34% │ LR: 0.00150000 │\n",
      "│ Epoch 31 │ Train Loss: 0.7684 │ Val Loss: 2.6631 │ F1: 27.16% │ LR: 0.00150000 │\n",
      "│ Epoch 32 │ Train Loss: 0.7717 │ Val Loss: 3.0255 │ F1: 28.62% │ LR: 0.00150000 │\n",
      "│ Epoch 33 │ Train Loss: 0.7666 │ Val Loss: 3.0044 │ F1: 26.89% │ LR: 0.00150000 │\n",
      "│ Epoch 34 │ Train Loss: 0.7439 │ Val Loss: 3.1393 │ F1: 28.70% │ LR: 0.00150000 │\n",
      "│ Epoch 35 │ Train Loss: 0.7683 │ Val Loss: 3.1807 │ F1: 29.46% │ LR: 0.00150000 │\n",
      "│ Epoch 36 │ Train Loss: 0.7445 │ Val Loss: 3.2150 │ F1: 29.19% │ LR: 0.00150000 │\n",
      "│ Epoch 37 │ Train Loss: 0.7647 │ Val Loss: 3.1333 │ F1: 28.89% │ LR: 0.00150000 │\n",
      "│ Epoch 38 │ Train Loss: 0.7227 │ Val Loss: 3.0226 │ F1: 29.75% │ LR: 0.00150000 │\n",
      "│ Epoch 39 │ Train Loss: 0.7595 │ Val Loss: 3.2383 │ F1: 31.65% │ LR: 0.00150000 │\n",
      "│ Epoch 40 │ Train Loss: 0.7144 │ Val Loss: 3.2310 │ F1: 30.05% │ LR: 0.00150000 │\n",
      "│ Epoch 41 │ Train Loss: 0.7200 │ Val Loss: 3.3578 │ F1: 31.39% │ LR: 0.00150000 │\n",
      "│ Epoch 42 │ Train Loss: 0.7301 │ Val Loss: 3.4814 │ F1: 32.28% │ LR: 0.00150000 │\n",
      "│ Epoch 43 │ Train Loss: 0.7268 │ Val Loss: 3.2144 │ F1: 32.14% │ LR: 0.00150000 │\n",
      "│ Epoch 44 │ Train Loss: 0.7107 │ Val Loss: 2.7243 │ F1: 30.57% │ LR: 0.00150000 │\n",
      "│ Epoch 45 │ Train Loss: 0.7276 │ Val Loss: 3.1668 │ F1: 29.96% │ LR: 0.00150000 │\n",
      "│ Epoch 46 │ Train Loss: 0.7344 │ Val Loss: 3.3487 │ F1: 30.60% │ LR: 0.00150000 │\n",
      "│ Epoch 47 │ Train Loss: 0.7023 │ Val Loss: 3.3440 │ F1: 32.67% │ LR: 0.00075000 │\n",
      "│ Epoch 48 │ Train Loss: 0.7251 │ Val Loss: 3.3586 │ F1: 32.31% │ LR: 0.00075000 │\n",
      "│ Epoch 49 │ Train Loss: 0.7014 │ Val Loss: 3.4775 │ F1: 31.68% │ LR: 0.00075000 │\n",
      "│ Epoch 50 │ Train Loss: 0.7009 │ Val Loss: 3.5685 │ F1: 33.47% │ LR: 0.00075000 │\n",
      "│ Epoch 51 │ Train Loss: 0.7178 │ Val Loss: 3.7462 │ F1: 35.12% │ LR: 0.00075000 │\n",
      "│ Epoch 52 │ Train Loss: 0.6925 │ Val Loss: 3.6941 │ F1: 34.79% │ LR: 0.00075000 │\n",
      "│ Epoch 53 │ Train Loss: 0.7220 │ Val Loss: 3.6712 │ F1: 33.42% │ LR: 0.00075000 │\n",
      "│ Epoch 54 │ Train Loss: 0.6986 │ Val Loss: 3.7431 │ F1: 33.67% │ LR: 0.00075000 │\n",
      "│ Epoch 55 │ Train Loss: 0.7004 │ Val Loss: 3.6409 │ F1: 32.68% │ LR: 0.00075000 │\n",
      "│ Epoch 56 │ Train Loss: 0.7040 │ Val Loss: 3.7744 │ F1: 34.64% │ LR: 0.00037500 │\n",
      "│ Epoch 57 │ Train Loss: 0.7313 │ Val Loss: 3.8916 │ F1: 35.60% │ LR: 0.00037500 │\n",
      "│ Epoch 58 │ Train Loss: 0.7248 │ Val Loss: 3.9270 │ F1: 35.47% │ LR: 0.00037500 │\n",
      "│ Epoch 59 │ Train Loss: 0.6845 │ Val Loss: 3.9031 │ F1: 35.52% │ LR: 0.00037500 │\n",
      "│ Epoch 60 │ Train Loss: 0.7513 │ Val Loss: 3.9275 │ F1: 34.96% │ LR: 0.00037500 │\n",
      "│ Epoch 61 │ Train Loss: 0.6720 │ Val Loss: 3.9854 │ F1: 35.74% │ LR: 0.00037500 │\n",
      "│ Epoch 62 │ Train Loss: 0.6859 │ Val Loss: 3.9990 │ F1: 35.33% │ LR: 0.00037500 │\n",
      "│ Epoch 63 │ Train Loss: 0.6926 │ Val Loss: 4.0007 │ F1: 35.59% │ LR: 0.00037500 │\n",
      "│ Epoch 64 │ Train Loss: 0.7032 │ Val Loss: 4.0939 │ F1: 36.99% │ LR: 0.00037500 │\n",
      "│ Epoch 65 │ Train Loss: 0.7083 │ Val Loss: 4.1286 │ F1: 36.00% │ LR: 0.00037500 │\n",
      "│ Epoch 66 │ Train Loss: 0.6744 │ Val Loss: 4.1682 │ F1: 36.72% │ LR: 0.00037500 │\n",
      "│ Epoch 67 │ Train Loss: 0.7034 │ Val Loss: 4.1451 │ F1: 36.70% │ LR: 0.00037500 │\n",
      "│ Epoch 68 │ Train Loss: 0.7030 │ Val Loss: 4.1903 │ F1: 36.99% │ LR: 0.00037500 │\n",
      "│ Epoch 69 │ Train Loss: 0.6895 │ Val Loss: 4.1860 │ F1: 36.80% │ LR: 0.00018750 │\n",
      "│ Epoch 70 │ Train Loss: 0.6755 │ Val Loss: 4.2271 │ F1: 37.33% │ LR: 0.00018750 │\n",
      "│ Epoch 71 │ Train Loss: 0.6783 │ Val Loss: 4.2801 │ F1: 37.47% │ LR: 0.00018750 │\n",
      "│ Epoch 72 │ Train Loss: 0.6886 │ Val Loss: 4.2497 │ F1: 37.47% │ LR: 0.00018750 │\n"
     ]
    }
   ],
   "source": [
    "from attention import LSTM\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "WARMUP_EPOCHS = 12\n",
    "EPOCHS = 72\n",
    "LR = 3e-3\n",
    "WD = 1e-3\n",
    "\n",
    "FACTOR = 0.5\n",
    "PATIENCE = 5\n",
    "\n",
    "config = {\n",
    "    'embed_dim': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_classes': 28,\n",
    "    'dropout_rate_emb': 0.1,\n",
    "    'dropout_rate_linear': 0.5,\n",
    "    'dropout_rate_hidden': 0.1,\n",
    "    # 'num_layers': 2\n",
    "}\n",
    "\n",
    "config['vocab_size'] = len(vocab.dictionary)\n",
    "config['pad_idx'] = vocab.pad_idx\n",
    "\n",
    "gt = torch.stack([torch.tensor(label) for label in y_test], dim=0).cpu()\n",
    "model = LSTM(**config).to(device)\n",
    "\n",
    "train_config = {\n",
    "    'warmup_epochs': WARMUP_EPOCHS,\n",
    "    'epochs': EPOCHS,\n",
    "    'model': model,\n",
    "    'optim': AdamW(model.parameters(), lr=LR, weight_decay=WD),\n",
    "    'criterion': BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'val_gt': gt,\n",
    "}\n",
    "\n",
    "\n",
    "total_warmup_steps = WARMUP_EPOCHS * len(train_loader) \n",
    "\n",
    "warmup_scheduler = lr_scheduler.LinearLR(\n",
    "    train_config['optim'],\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=total_warmup_steps\n",
    ")\n",
    "\n",
    "\n",
    "plateau_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    train_config['optim'],\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "train_config['warmup_scheduler'] = warmup_scheduler\n",
    "train_config['plateau_scheduler'] = plateau_scheduler\n",
    "\n",
    "print('Training model...')\n",
    "train(**train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phQ-ka4mp0oS",
   "metadata": {
    "id": "phQ-ka4mp0oS"
   },
   "source": [
    "__Задание 7 (1 балл).__ Добавьте в вашу реализации возможность увеличивать число слоев LSTM. Обучите модель с двумя слоями и замерьте качество. Сделайте выводы: стоит ли увеличивать размер модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee7c7177",
   "metadata": {
    "id": "ee7c7177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "│ Epoch 01 │ Train Loss: 1.2963 │ Val Loss: 1.3914 │ F1: 8.87% │ LR: 0.00027750 │\n",
      "│ Epoch 02 │ Train Loss: 1.3213 │ Val Loss: 1.3915 │ F1: 9.70% │ LR: 0.00052500 │\n",
      "│ Epoch 03 │ Train Loss: 1.3005 │ Val Loss: 1.3908 │ F1: 12.87% │ LR: 0.00077250 │\n",
      "│ Epoch 04 │ Train Loss: 1.2832 │ Val Loss: 1.3659 │ F1: 11.34% │ LR: 0.00102000 │\n",
      "│ Epoch 05 │ Train Loss: 1.2154 │ Val Loss: 1.2639 │ F1: 12.31% │ LR: 0.00126750 │\n",
      "│ Epoch 06 │ Train Loss: 1.1704 │ Val Loss: 1.3678 │ F1: 12.40% │ LR: 0.00151500 │\n",
      "│ Epoch 07 │ Train Loss: 1.1154 │ Val Loss: 1.3889 │ F1: 15.13% │ LR: 0.00176250 │\n",
      "│ Epoch 08 │ Train Loss: 1.0845 │ Val Loss: 1.4447 │ F1: 13.34% │ LR: 0.00201000 │\n",
      "│ Epoch 09 │ Train Loss: 1.0953 │ Val Loss: 1.3842 │ F1: 14.68% │ LR: 0.00225750 │\n",
      "│ Epoch 10 │ Train Loss: 1.0755 │ Val Loss: 1.4109 │ F1: 13.21% │ LR: 0.00250500 │\n",
      "│ Epoch 11 │ Train Loss: 1.0331 │ Val Loss: 1.3295 │ F1: 15.55% │ LR: 0.00275250 │\n",
      "│ Epoch 12 │ Train Loss: 1.0234 │ Val Loss: 1.7213 │ F1: 17.38% │ LR: 0.00300000 │\n",
      "│ Epoch 13 │ Train Loss: 0.9858 │ Val Loss: 1.6688 │ F1: 16.27% │ LR: 0.00300000 │\n",
      "│ Epoch 14 │ Train Loss: 1.0126 │ Val Loss: 1.4472 │ F1: 18.99% │ LR: 0.00300000 │\n",
      "│ Epoch 15 │ Train Loss: 0.9470 │ Val Loss: 1.6342 │ F1: 16.92% │ LR: 0.00300000 │\n",
      "│ Epoch 16 │ Train Loss: 0.9197 │ Val Loss: 1.4799 │ F1: 18.63% │ LR: 0.00300000 │\n",
      "│ Epoch 17 │ Train Loss: 0.9380 │ Val Loss: 1.6261 │ F1: 18.33% │ LR: 0.00300000 │\n",
      "│ Epoch 18 │ Train Loss: 0.9056 │ Val Loss: 1.4377 │ F1: 20.27% │ LR: 0.00300000 │\n",
      "│ Epoch 19 │ Train Loss: 0.8473 │ Val Loss: 1.6992 │ F1: 21.49% │ LR: 0.00300000 │\n",
      "│ Epoch 20 │ Train Loss: 0.8891 │ Val Loss: 1.5572 │ F1: 22.32% │ LR: 0.00300000 │\n",
      "│ Epoch 21 │ Train Loss: 0.8767 │ Val Loss: 1.6548 │ F1: 23.82% │ LR: 0.00300000 │\n",
      "│ Epoch 22 │ Train Loss: 0.8675 │ Val Loss: 1.8554 │ F1: 24.71% │ LR: 0.00300000 │\n",
      "│ Epoch 23 │ Train Loss: 0.8543 │ Val Loss: 1.8768 │ F1: 25.33% │ LR: 0.00300000 │\n",
      "│ Epoch 24 │ Train Loss: 0.8341 │ Val Loss: 1.8202 │ F1: 22.88% │ LR: 0.00300000 │\n",
      "│ Epoch 25 │ Train Loss: 0.8042 │ Val Loss: 1.7748 │ F1: 23.24% │ LR: 0.00300000 │\n",
      "│ Epoch 26 │ Train Loss: 0.7981 │ Val Loss: 1.8691 │ F1: 22.28% │ LR: 0.00300000 │\n",
      "│ Epoch 27 │ Train Loss: 0.7900 │ Val Loss: 1.7016 │ F1: 22.92% │ LR: 0.00300000 │\n",
      "│ Epoch 28 │ Train Loss: 0.7712 │ Val Loss: 1.9371 │ F1: 28.33% │ LR: 0.00150000 │\n",
      "│ Epoch 29 │ Train Loss: 0.7917 │ Val Loss: 1.9119 │ F1: 26.38% │ LR: 0.00150000 │\n",
      "│ Epoch 30 │ Train Loss: 0.7894 │ Val Loss: 1.9106 │ F1: 28.58% │ LR: 0.00150000 │\n",
      "│ Epoch 31 │ Train Loss: 0.7633 │ Val Loss: 2.0256 │ F1: 29.79% │ LR: 0.00150000 │\n",
      "│ Epoch 32 │ Train Loss: 0.7572 │ Val Loss: 2.0553 │ F1: 29.44% │ LR: 0.00150000 │\n",
      "│ Epoch 33 │ Train Loss: 0.7837 │ Val Loss: 2.3090 │ F1: 32.82% │ LR: 0.00150000 │\n",
      "│ Epoch 34 │ Train Loss: 0.7175 │ Val Loss: 2.1772 │ F1: 31.51% │ LR: 0.00150000 │\n",
      "│ Epoch 35 │ Train Loss: 0.7357 │ Val Loss: 2.2697 │ F1: 33.46% │ LR: 0.00150000 │\n",
      "│ Epoch 36 │ Train Loss: 0.7380 │ Val Loss: 2.2835 │ F1: 31.96% │ LR: 0.00150000 │\n",
      "│ Epoch 37 │ Train Loss: 0.7071 │ Val Loss: 2.3835 │ F1: 33.09% │ LR: 0.00150000 │\n",
      "│ Epoch 38 │ Train Loss: 0.7386 │ Val Loss: 2.4519 │ F1: 33.45% │ LR: 0.00150000 │\n",
      "│ Epoch 39 │ Train Loss: 0.7338 │ Val Loss: 2.5445 │ F1: 34.79% │ LR: 0.00150000 │\n",
      "│ Epoch 40 │ Train Loss: 0.7021 │ Val Loss: 2.5852 │ F1: 34.00% │ LR: 0.00150000 │\n",
      "│ Epoch 41 │ Train Loss: 0.7024 │ Val Loss: 2.5912 │ F1: 35.04% │ LR: 0.00150000 │\n",
      "│ Epoch 42 │ Train Loss: 0.7185 │ Val Loss: 2.6952 │ F1: 36.32% │ LR: 0.00150000 │\n",
      "│ Epoch 43 │ Train Loss: 0.7512 │ Val Loss: 2.8123 │ F1: 35.19% │ LR: 0.00150000 │\n",
      "│ Epoch 44 │ Train Loss: 0.7044 │ Val Loss: 2.8470 │ F1: 36.21% │ LR: 0.00150000 │\n",
      "│ Epoch 45 │ Train Loss: 0.6675 │ Val Loss: 2.8729 │ F1: 36.55% │ LR: 0.00150000 │\n",
      "│ Epoch 46 │ Train Loss: 0.6942 │ Val Loss: 2.7821 │ F1: 37.30% │ LR: 0.00150000 │\n",
      "│ Epoch 47 │ Train Loss: 0.6984 │ Val Loss: 2.6978 │ F1: 37.03% │ LR: 0.00150000 │\n",
      "│ Epoch 48 │ Train Loss: 0.6901 │ Val Loss: 2.6922 │ F1: 36.80% │ LR: 0.00150000 │\n",
      "│ Epoch 49 │ Train Loss: 0.6802 │ Val Loss: 3.0115 │ F1: 36.71% │ LR: 0.00150000 │\n",
      "│ Epoch 50 │ Train Loss: 0.7331 │ Val Loss: 2.6436 │ F1: 35.07% │ LR: 0.00150000 │\n",
      "│ Epoch 51 │ Train Loss: 0.6845 │ Val Loss: 2.6845 │ F1: 35.35% │ LR: 0.00075000 │\n",
      "│ Epoch 52 │ Train Loss: 0.7015 │ Val Loss: 2.7506 │ F1: 35.94% │ LR: 0.00075000 │\n",
      "│ Epoch 53 │ Train Loss: 0.6822 │ Val Loss: 2.8995 │ F1: 37.67% │ LR: 0.00075000 │\n",
      "│ Epoch 54 │ Train Loss: 0.7043 │ Val Loss: 3.0035 │ F1: 38.61% │ LR: 0.00075000 │\n",
      "│ Epoch 55 │ Train Loss: 0.6912 │ Val Loss: 2.8529 │ F1: 38.27% │ LR: 0.00075000 │\n",
      "│ Epoch 56 │ Train Loss: 0.6530 │ Val Loss: 3.0319 │ F1: 39.97% │ LR: 0.00075000 │\n",
      "│ Epoch 57 │ Train Loss: 0.6552 │ Val Loss: 3.0046 │ F1: 39.71% │ LR: 0.00075000 │\n",
      "│ Epoch 58 │ Train Loss: 0.6773 │ Val Loss: 2.9852 │ F1: 39.05% │ LR: 0.00075000 │\n",
      "│ Epoch 59 │ Train Loss: 0.6858 │ Val Loss: 3.0585 │ F1: 39.93% │ LR: 0.00075000 │\n",
      "│ Epoch 60 │ Train Loss: 0.6719 │ Val Loss: 3.2189 │ F1: 39.02% │ LR: 0.00075000 │\n",
      "│ Epoch 61 │ Train Loss: 0.7139 │ Val Loss: 3.1744 │ F1: 38.43% │ LR: 0.00037500 │\n",
      "│ Epoch 62 │ Train Loss: 0.6877 │ Val Loss: 3.1106 │ F1: 37.44% │ LR: 0.00037500 │\n",
      "│ Epoch 63 │ Train Loss: 0.6669 │ Val Loss: 3.1508 │ F1: 39.55% │ LR: 0.00037500 │\n",
      "│ Epoch 64 │ Train Loss: 0.6817 │ Val Loss: 3.2261 │ F1: 40.62% │ LR: 0.00037500 │\n",
      "│ Epoch 65 │ Train Loss: 0.6627 │ Val Loss: 3.2281 │ F1: 40.24% │ LR: 0.00037500 │\n",
      "│ Epoch 66 │ Train Loss: 0.6557 │ Val Loss: 3.2349 │ F1: 40.00% │ LR: 0.00037500 │\n",
      "│ Epoch 67 │ Train Loss: 0.6656 │ Val Loss: 3.2666 │ F1: 40.45% │ LR: 0.00037500 │\n",
      "│ Epoch 68 │ Train Loss: 0.6588 │ Val Loss: 3.3043 │ F1: 40.12% │ LR: 0.00037500 │\n",
      "│ Epoch 69 │ Train Loss: 0.6822 │ Val Loss: 3.3073 │ F1: 40.35% │ LR: 0.00018750 │\n",
      "│ Epoch 70 │ Train Loss: 0.6555 │ Val Loss: 3.3051 │ F1: 40.22% │ LR: 0.00018750 │\n",
      "│ Epoch 71 │ Train Loss: 0.6691 │ Val Loss: 3.3133 │ F1: 40.13% │ LR: 0.00018750 │\n",
      "│ Epoch 72 │ Train Loss: 0.6963 │ Val Loss: 3.3173 │ F1: 38.89% │ LR: 0.00018750 │\n"
     ]
    }
   ],
   "source": [
    "from deep import LSTM\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "WARMUP_EPOCHS = 12\n",
    "EPOCHS = 72\n",
    "LR = 3e-3\n",
    "WD = 1e-3\n",
    "\n",
    "FACTOR = 0.5\n",
    "PATIENCE = 5\n",
    "\n",
    "config = {\n",
    "    'embed_dim': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_classes': 28,\n",
    "    'dropout_rate_emb': 0.1,\n",
    "    'dropout_rate_linear': 0.5,\n",
    "    'dropout_rate_hidden': 0.1,\n",
    "    'num_layers': 2\n",
    "}\n",
    "\n",
    "config['vocab_size'] = len(vocab.dictionary)\n",
    "config['pad_idx'] = vocab.pad_idx\n",
    "\n",
    "gt = torch.stack([torch.tensor(label) for label in y_test], dim=0).cpu()\n",
    "model = LSTM(**config).to(device)\n",
    "\n",
    "train_config = {\n",
    "    'warmup_epochs': WARMUP_EPOCHS,\n",
    "    'epochs': EPOCHS,\n",
    "    'model': model,\n",
    "    'optim': AdamW(model.parameters(), lr=LR, weight_decay=WD),\n",
    "    'criterion': BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'val_gt': gt,\n",
    "}\n",
    "\n",
    "\n",
    "total_warmup_steps = WARMUP_EPOCHS * len(train_loader) \n",
    "\n",
    "warmup_scheduler = lr_scheduler.LinearLR(\n",
    "    train_config['optim'],\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=total_warmup_steps\n",
    ")\n",
    "\n",
    "\n",
    "plateau_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    train_config['optim'],\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "train_config['warmup_scheduler'] = warmup_scheduler\n",
    "train_config['plateau_scheduler'] = plateau_scheduler\n",
    "\n",
    "print('Training model...')\n",
    "train(**train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476026ee-7aef-466e-bdc8-8daae1573dfc",
   "metadata": {},
   "source": [
    "# BPE with RUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b478599-b473-4490-b289-8cb5bdfc1bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:00<00:00, 2235062.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:05<00:00, 419.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2431/2431 [00:00<00:00, 55646.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 608/608 [00:01<00:00, 424.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 608/608 [00:00<00:00, 55123.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "│ Epoch 01 │ Train Loss: 1.2978 │ Val Loss: 1.3922 │ F1: 8.94% │ LR: 0.00027750 │\n",
      "│ Epoch 02 │ Train Loss: 1.2960 │ Val Loss: 1.3898 │ F1: 9.92% │ LR: 0.00052500 │\n",
      "│ Epoch 03 │ Train Loss: 1.2976 │ Val Loss: 1.3894 │ F1: 10.14% │ LR: 0.00077250 │\n",
      "│ Epoch 04 │ Train Loss: 1.2784 │ Val Loss: 1.3492 │ F1: 10.52% │ LR: 0.00102000 │\n",
      "│ Epoch 05 │ Train Loss: 1.2072 │ Val Loss: 1.3541 │ F1: 9.55% │ LR: 0.00126750 │\n",
      "│ Epoch 06 │ Train Loss: 1.2093 │ Val Loss: 1.3664 │ F1: 11.96% │ LR: 0.00151500 │\n",
      "│ Epoch 07 │ Train Loss: 1.1477 │ Val Loss: 1.3416 │ F1: 14.15% │ LR: 0.00176250 │\n",
      "│ Epoch 08 │ Train Loss: 1.1012 │ Val Loss: 1.3409 │ F1: 11.91% │ LR: 0.00201000 │\n",
      "│ Epoch 09 │ Train Loss: 1.0840 │ Val Loss: 1.4369 │ F1: 18.13% │ LR: 0.00225750 │\n",
      "│ Epoch 10 │ Train Loss: 1.0413 │ Val Loss: 1.5317 │ F1: 13.42% │ LR: 0.00250500 │\n",
      "│ Epoch 11 │ Train Loss: 1.0719 │ Val Loss: 1.2693 │ F1: 14.65% │ LR: 0.00275250 │\n",
      "│ Epoch 12 │ Train Loss: 1.0736 │ Val Loss: 1.4570 │ F1: 16.12% │ LR: 0.00300000 │\n",
      "│ Epoch 13 │ Train Loss: 0.9942 │ Val Loss: 1.5460 │ F1: 18.64% │ LR: 0.00300000 │\n",
      "│ Epoch 14 │ Train Loss: 0.9607 │ Val Loss: 1.4947 │ F1: 15.20% │ LR: 0.00300000 │\n",
      "│ Epoch 15 │ Train Loss: 0.9573 │ Val Loss: 1.5712 │ F1: 16.98% │ LR: 0.00300000 │\n",
      "│ Epoch 16 │ Train Loss: 1.0220 │ Val Loss: 1.6020 │ F1: 15.09% │ LR: 0.00300000 │\n",
      "│ Epoch 17 │ Train Loss: 0.9036 │ Val Loss: 1.6658 │ F1: 20.79% │ LR: 0.00300000 │\n",
      "│ Epoch 18 │ Train Loss: 0.8838 │ Val Loss: 1.6882 │ F1: 19.91% │ LR: 0.00300000 │\n",
      "│ Epoch 19 │ Train Loss: 0.8511 │ Val Loss: 1.6525 │ F1: 22.24% │ LR: 0.00300000 │\n",
      "│ Epoch 20 │ Train Loss: 0.8287 │ Val Loss: 1.5804 │ F1: 20.03% │ LR: 0.00300000 │\n",
      "│ Epoch 21 │ Train Loss: 0.8349 │ Val Loss: 1.7129 │ F1: 24.14% │ LR: 0.00300000 │\n",
      "│ Epoch 22 │ Train Loss: 0.8210 │ Val Loss: 1.8380 │ F1: 25.28% │ LR: 0.00300000 │\n",
      "│ Epoch 23 │ Train Loss: 0.7963 │ Val Loss: 1.9822 │ F1: 26.51% │ LR: 0.00300000 │\n",
      "│ Epoch 24 │ Train Loss: 0.7894 │ Val Loss: 2.0456 │ F1: 27.85% │ LR: 0.00300000 │\n",
      "│ Epoch 25 │ Train Loss: 0.7860 │ Val Loss: 2.1016 │ F1: 26.98% │ LR: 0.00300000 │\n",
      "│ Epoch 26 │ Train Loss: 0.7652 │ Val Loss: 2.0159 │ F1: 25.35% │ LR: 0.00300000 │\n",
      "│ Epoch 27 │ Train Loss: 0.7484 │ Val Loss: 2.0882 │ F1: 27.18% │ LR: 0.00300000 │\n",
      "│ Epoch 28 │ Train Loss: 0.7593 │ Val Loss: 2.2434 │ F1: 28.37% │ LR: 0.00300000 │\n",
      "│ Epoch 29 │ Train Loss: 0.7965 │ Val Loss: 2.0722 │ F1: 25.72% │ LR: 0.00300000 │\n",
      "│ Epoch 30 │ Train Loss: 0.7844 │ Val Loss: 2.2493 │ F1: 28.45% │ LR: 0.00300000 │\n",
      "│ Epoch 31 │ Train Loss: 0.7341 │ Val Loss: 2.3107 │ F1: 27.82% │ LR: 0.00300000 │\n",
      "│ Epoch 32 │ Train Loss: 0.7460 │ Val Loss: 2.2139 │ F1: 30.00% │ LR: 0.00300000 │\n",
      "│ Epoch 33 │ Train Loss: 0.7502 │ Val Loss: 2.2703 │ F1: 29.48% │ LR: 0.00300000 │\n",
      "│ Epoch 34 │ Train Loss: 0.7633 │ Val Loss: 2.3610 │ F1: 28.63% │ LR: 0.00300000 │\n",
      "│ Epoch 35 │ Train Loss: 0.7525 │ Val Loss: 2.3072 │ F1: 28.60% │ LR: 0.00300000 │\n",
      "│ Epoch 36 │ Train Loss: 0.7365 │ Val Loss: 2.3999 │ F1: 31.01% │ LR: 0.00300000 │\n",
      "│ Epoch 37 │ Train Loss: 0.7310 │ Val Loss: 2.4718 │ F1: 30.95% │ LR: 0.00300000 │\n",
      "│ Epoch 38 │ Train Loss: 0.7227 │ Val Loss: 2.4675 │ F1: 29.88% │ LR: 0.00300000 │\n",
      "│ Epoch 39 │ Train Loss: 0.6874 │ Val Loss: 2.5245 │ F1: 29.53% │ LR: 0.00300000 │\n",
      "│ Epoch 40 │ Train Loss: 0.6871 │ Val Loss: 2.6303 │ F1: 29.86% │ LR: 0.00300000 │\n",
      "│ Epoch 41 │ Train Loss: 0.6735 │ Val Loss: 2.5760 │ F1: 29.04% │ LR: 0.00150000 │\n",
      "│ Epoch 42 │ Train Loss: 0.7119 │ Val Loss: 2.6472 │ F1: 32.92% │ LR: 0.00150000 │\n",
      "│ Epoch 43 │ Train Loss: 0.7005 │ Val Loss: 2.7410 │ F1: 33.38% │ LR: 0.00150000 │\n",
      "│ Epoch 44 │ Train Loss: 0.6911 │ Val Loss: 2.8816 │ F1: 33.92% │ LR: 0.00150000 │\n",
      "│ Epoch 45 │ Train Loss: 0.6953 │ Val Loss: 2.9470 │ F1: 33.94% │ LR: 0.00150000 │\n",
      "│ Epoch 46 │ Train Loss: 0.6995 │ Val Loss: 2.9876 │ F1: 32.75% │ LR: 0.00150000 │\n",
      "│ Epoch 47 │ Train Loss: 0.6989 │ Val Loss: 3.0919 │ F1: 33.81% │ LR: 0.00150000 │\n",
      "│ Epoch 48 │ Train Loss: 0.6796 │ Val Loss: 3.1600 │ F1: 35.73% │ LR: 0.00150000 │\n",
      "│ Epoch 49 │ Train Loss: 0.6945 │ Val Loss: 3.0593 │ F1: 34.39% │ LR: 0.00150000 │\n",
      "│ Epoch 50 │ Train Loss: 0.6849 │ Val Loss: 3.1157 │ F1: 34.63% │ LR: 0.00150000 │\n",
      "│ Epoch 51 │ Train Loss: 0.6568 │ Val Loss: 3.0530 │ F1: 33.93% │ LR: 0.00150000 │\n",
      "│ Epoch 52 │ Train Loss: 0.6568 │ Val Loss: 3.1268 │ F1: 34.32% │ LR: 0.00150000 │\n",
      "│ Epoch 53 │ Train Loss: 0.6603 │ Val Loss: 3.1806 │ F1: 34.84% │ LR: 0.00075000 │\n",
      "│ Epoch 54 │ Train Loss: 0.6668 │ Val Loss: 3.2851 │ F1: 35.94% │ LR: 0.00075000 │\n",
      "│ Epoch 55 │ Train Loss: 0.6595 │ Val Loss: 3.3110 │ F1: 35.47% │ LR: 0.00075000 │\n",
      "│ Epoch 56 │ Train Loss: 0.6815 │ Val Loss: 3.3041 │ F1: 34.83% │ LR: 0.00075000 │\n",
      "│ Epoch 57 │ Train Loss: 0.7090 │ Val Loss: 3.3647 │ F1: 35.11% │ LR: 0.00075000 │\n",
      "│ Epoch 58 │ Train Loss: 0.6714 │ Val Loss: 3.4351 │ F1: 35.39% │ LR: 0.00075000 │\n",
      "│ Epoch 59 │ Train Loss: 0.6542 │ Val Loss: 3.4293 │ F1: 35.13% │ LR: 0.00037500 │\n",
      "│ Epoch 60 │ Train Loss: 0.6522 │ Val Loss: 3.4275 │ F1: 35.49% │ LR: 0.00037500 │\n",
      "│ Epoch 61 │ Train Loss: 0.6815 │ Val Loss: 3.4259 │ F1: 35.50% │ LR: 0.00037500 │\n",
      "│ Epoch 62 │ Train Loss: 0.6633 │ Val Loss: 3.4097 │ F1: 34.92% │ LR: 0.00037500 │\n",
      "│ Epoch 63 │ Train Loss: 0.6904 │ Val Loss: 3.4393 │ F1: 35.42% │ LR: 0.00018750 │\n",
      "│ Epoch 64 │ Train Loss: 0.6622 │ Val Loss: 3.4602 │ F1: 35.52% │ LR: 0.00018750 │\n",
      "│ Epoch 65 │ Train Loss: 0.7201 │ Val Loss: 3.4648 │ F1: 35.77% │ LR: 0.00018750 │\n",
      "│ Epoch 66 │ Train Loss: 0.6442 │ Val Loss: 3.4719 │ F1: 35.71% │ LR: 0.00018750 │\n",
      "│ Epoch 67 │ Train Loss: 0.6694 │ Val Loss: 3.4681 │ F1: 35.24% │ LR: 0.00009375 │\n",
      "│ Epoch 68 │ Train Loss: 0.6702 │ Val Loss: 3.4722 │ F1: 35.25% │ LR: 0.00009375 │\n",
      "│ Epoch 69 │ Train Loss: 0.7138 │ Val Loss: 3.4731 │ F1: 35.13% │ LR: 0.00009375 │\n",
      "│ Epoch 70 │ Train Loss: 0.6837 │ Val Loss: 3.4751 │ F1: 35.16% │ LR: 0.00009375 │\n",
      "│ Epoch 71 │ Train Loss: 0.6491 │ Val Loss: 3.4781 │ F1: 35.15% │ LR: 0.00004688 │\n",
      "│ Epoch 72 │ Train Loss: 0.6491 │ Val Loss: 3.4799 │ F1: 35.15% │ LR: 0.00004688 │\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rnn import RNN\n",
    "from deep import LSTM\n",
    "from torchrnn import TorchRNN\n",
    "# from vocab import Vocab\n",
    "from vocab_bpe import BPEVocab\n",
    "from dataset import TextDataset, collate, load_biotech, load_preprocessed\n",
    "from train import train\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "WARMUP_EPOCHS = 12\n",
    "EPOCHS = 72\n",
    "LR = 3e-3\n",
    "WD = 1e-3\n",
    "\n",
    "FACTOR = 0.5\n",
    "PATIENCE = 5\n",
    "\n",
    "config = {\n",
    "    'embed_dim': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_classes': 28,\n",
    "    'dropout_rate_emb': 0.1,\n",
    "    'dropout_rate_linear': 0.5,\n",
    "    'dropout_rate_hidden': 0.1,\n",
    "    'num_layers': 2\n",
    "}\n",
    "\n",
    "print('Loading dataset...')\n",
    "# dataset, labels = load_biotech()\n",
    "dataset, labels = load_preprocessed()\n",
    "\n",
    "device = 'cuda:0'\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(\n",
    "    dataset.text.values,\n",
    "    dataset.encoded_labels.values,\n",
    "    test_size=0.2,  # do not change this\n",
    "    random_state=0  # do not change this\n",
    ")\n",
    "\n",
    "print('Building vocab...')\n",
    "# vocab = Vocab(texts_train, no_below=26, no_above=0.68, max_tokens=36, clean=False)\n",
    "vocab = BPEVocab(texts_train, vocab_size=4000, max_tokens=100, clean=False)\n",
    "\n",
    "# print('Vocab size:', len(vocab.dictionary))\n",
    "\n",
    "print('Tokenizing ...')\n",
    "train_dataset = TextDataset(texts_train, y_train, vocab, device)\n",
    "test_dataset = TextDataset(texts_test, y_test, vocab, device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate(batch, vocab.pad_idx)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate(batch, vocab.pad_idx)\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor((y_train.shape[0] - y_train.sum(0)) / (y_train.sum(0) + 1e-6), dtype=torch.float32, device=device) # type: ignore\n",
    "# class_weights = torch.tensor((y_train.shape[0]) / (y_train.sum(0) + 1e-6), dtype=torch.float32, device=device)\n",
    "\n",
    "config['vocab_size'] = vocab.tokenizer.vocab_size()\n",
    "config['pad_idx'] = vocab.pad_idx\n",
    "\n",
    "gt = torch.stack([torch.tensor(label) for label in y_test], dim=0).cpu()\n",
    "model = LSTM(**config).to(device)\n",
    "\n",
    "train_config = {\n",
    "    'warmup_epochs': WARMUP_EPOCHS,\n",
    "    'epochs': EPOCHS,\n",
    "    'model': model,\n",
    "    'optim': AdamW(model.parameters(), lr=LR, weight_decay=WD),\n",
    "    'criterion': BCEWithLogitsLoss(pos_weight=class_weights),\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'val_gt': gt,\n",
    "}\n",
    "\n",
    "\n",
    "total_warmup_steps = WARMUP_EPOCHS * len(train_loader) \n",
    "\n",
    "warmup_scheduler = lr_scheduler.LinearLR(\n",
    "    train_config['optim'],\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=total_warmup_steps\n",
    ")\n",
    "\n",
    "\n",
    "plateau_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    train_config['optim'],\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "train_config['warmup_scheduler'] = warmup_scheduler\n",
    "train_config['plateau_scheduler'] = plateau_scheduler\n",
    "\n",
    "print('Training model...')\n",
    "train(**train_config)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP LSTM",
   "language": "python",
   "name": "hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12b0627d4aaf46c0adc64b442bf88d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d5b2e090c51406e953b4eec4b0b91ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "282f83858a424e2ea76990eb957dc5a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32808478ae8c4242beb79f0272ea6b1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34e8d1401c0e4dc1a8e71bbad7c2f74d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b23f3b8b7247491c8d5e3ead7f54d886",
      "placeholder": "​",
      "style": "IPY_MODEL_cb632291897f4f9db86a00a5a71ca35f",
      "value": " 40/40 [36:41&lt;00:00, 51.61s/it]"
     }
    },
    "3735627f227d4b4f927955113111409f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47f4f11bc6984b96ac3c3875d733f0ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc4f687f9d5940aba074e2bb41581c93",
      "max": 40,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e10fd6d1a6c47a9ac34a47ae5ba708b",
      "value": 40
     }
    },
    "4aab16bb20824688aadbd23460adad9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f65eec1b45de42e59fb9e24b99aad917",
       "IPY_MODEL_47f4f11bc6984b96ac3c3875d733f0ba",
       "IPY_MODEL_f58fddb1bf414071b0523701a619ad71"
      ],
      "layout": "IPY_MODEL_32808478ae8c4242beb79f0272ea6b1f"
     }
    },
    "4de9492961d841aa9f3d7bc629911296": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67ae0c089c4a426db3b52976fae1a9dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e10fd6d1a6c47a9ac34a47ae5ba708b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b23f3b8b7247491c8d5e3ead7f54d886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc4165ff8fc3480fb1590b6ecd39fb4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cba16e32a9df4b1b89b4f7066945fc42",
       "IPY_MODEL_e8f0522f19c44066b5a78ded999f050a",
       "IPY_MODEL_34e8d1401c0e4dc1a8e71bbad7c2f74d"
      ],
      "layout": "IPY_MODEL_282f83858a424e2ea76990eb957dc5a0"
     }
    },
    "cb632291897f4f9db86a00a5a71ca35f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cba16e32a9df4b1b89b4f7066945fc42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67ae0c089c4a426db3b52976fae1a9dc",
      "placeholder": "​",
      "style": "IPY_MODEL_12b0627d4aaf46c0adc64b442bf88d0a",
      "value": "100%"
     }
    },
    "d7ed88f49793494bbdb3c2fffc01b216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc4f687f9d5940aba074e2bb41581c93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7876fd73da349ea873c137c63d8d528": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e8f0522f19c44066b5a78ded999f050a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4de9492961d841aa9f3d7bc629911296",
      "max": 40,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7876fd73da349ea873c137c63d8d528",
      "value": 40
     }
    },
    "f58fddb1bf414071b0523701a619ad71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d5b2e090c51406e953b4eec4b0b91ad",
      "placeholder": "​",
      "style": "IPY_MODEL_3735627f227d4b4f927955113111409f",
      "value": " 40/40 [1:08:10&lt;00:00, 102.17s/it]"
     }
    },
    "f65eec1b45de42e59fb9e24b99aad917": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f67dc08a01ac40ad98ed553fe6b7e948",
      "placeholder": "​",
      "style": "IPY_MODEL_d7ed88f49793494bbdb3c2fffc01b216",
      "value": "100%"
     }
    },
    "f67dc08a01ac40ad98ed553fe6b7e948": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
